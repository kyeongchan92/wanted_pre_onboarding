{"cells":[{"cell_type":"markdown","metadata":{"id":"592U6lXs3d2t"},"source":["# Week2_4 Assignment\n","\n","## [BASIC](#Basic) \n","- 커스텀 모듈(`helper.py`)에서 **클래스와 함수를 임포트**할 수 있다.\n","- **autograd**의 개념 복습\n","\n","\n","## [CHALLENGE](#Challenge)\n","- train() 함수에 **epoch, scheduler, grad_clipping**을 추가할 수 있다.\n","- **validate() 함수를 구현**할 수 있다.\n","\n","\n","## [ADVANCED](#Advanced)\n","- train() 함수를 사용해 데이터를 **4 epoch 학습**할 수 있다. \n","- **predict 함수를 구현**할 수 있다. \n","- **evaluation metric 구현**할 수 있다. \n","    - accuracy\n","\n","\n","\n","### Reference\n","- [Pytorch Autograd Explain official document](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:47.370876Z","start_time":"2022-02-02T04:01:46.520392Z"},"id":"KSX-wQA1RD1h"},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","import numpy as np \n","import torch\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:47.375658Z","start_time":"2022-02-02T04:01:47.372242Z"},"id":"MH7RJjtZXOHf"},"outputs":[],"source":["# set seed\n","seed = 7777\n","random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-01-31T13:07:00.849353Z","start_time":"2022-01-31T13:06:56.187962Z"},"id":"62plMahMWr0U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646298284735,"user_tz":-540,"elapsed":6809,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"f3c8f045-35f8-43c7-bb69-1d640369c0b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 14.4 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 71.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 64.1 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 57.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.16.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6WagJcj-Ud4L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646298302958,"user_tz":-540,"elapsed":18229,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"3cc64732-d335-4ad0-bacd-5515c3930022"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnETqIqdVApF"},"outputs":[],"source":["# 어제 자신이 구현한 helper.py 모듈 경로를 입력\n","sys.path.append('G:\\내 드라이브\\015GithubRepos\\wanted_pre_onboarding')"]},{"cell_type":"code","source":["cd /content/drive/MyDrive/015GithubRepos/wanted_pre_onboarding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DSmcVNvJYjlS","executionInfo":{"status":"ok","timestamp":1646298303453,"user_tz":-540,"elapsed":503,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"bda92e3d-dbb6-40f1-b15a-f414dcc87dee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/015GithubRepos/wanted_pre_onboarding\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:49.735578Z","start_time":"2022-02-02T04:01:49.475969Z"},"id":"N84mZeYMUFxJ"},"outputs":[],"source":["# helper 모듈을 import하면 이전에 구현했던 다양한 함수 및 클래스를 사용할 수 있음 \n","# 함수: set_device()\n","# 함수: custom_collate_fn() \n","# 클래스: CustomDataset\n","# 클래스: CustomClassifier\n","\n","from helper import *\n","from torch.utils.data import RandomSampler, SequentialSampler"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:49.771743Z","start_time":"2022-02-02T04:01:49.736866Z"},"id":"oR5EWmh5UFxK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646298306004,"user_tz":-540,"elapsed":14,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"15956f5c-eb37-4a49-d71c-b3948bf587e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["# available GPUs : 1\n","GPU name : Tesla T4\n","cuda\n","device: cuda\n"]}],"source":["# device\n","device = set_device()\n","print(f\"device: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"pkNxrCV45Q3m"},"source":["## Basic"]},{"cell_type":"markdown","metadata":{"id":"9YBUQykS5Q3n"},"source":["### 모듈에서 클래스와 함수를 임포트해 다음을 구현\n","- train_dataset, train_dataloader\n","- valid_dataset, valid_dataloader\n","- test_dataset, test_dataloader"]},{"cell_type":"code","source":["# train dataframe 다운로드\n","!wget https://raw.githubusercontent.com/ChristinaROK/PreOnboarding_AI_assets/e56006adfac42f8a2975db0ebbe60eacbe1c6b11/data/sample_df.csv"],"metadata":{"id":"cjNksUEwGACb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646298306397,"user_tz":-540,"elapsed":404,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"0253dd00-59f2-444f-964b-6740b6872b01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-03-03 09:05:06--  https://raw.githubusercontent.com/ChristinaROK/PreOnboarding_AI_assets/e56006adfac42f8a2975db0ebbe60eacbe1c6b11/data/sample_df.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 971625 (949K) [text/plain]\n","Saving to: ‘sample_df.csv.8’\n","\n","sample_df.csv.8     100%[===================>] 948.85K  --.-KB/s    in 0.02s   \n","\n","2022-03-03 09:05:06 (47.8 MB/s) - ‘sample_df.csv.8’ saved [971625/971625]\n","\n"]}]},{"cell_type":"code","source":["# test dataframe 다운로드\n","!wget https://raw.githubusercontent.com/ChristinaROK/PreOnboarding_AI_assets/main/data/sample_df_test.csv"],"metadata":{"id":"kXfk8ZEHGB0v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646297920712,"user_tz":-540,"elapsed":389,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"bdc1fd9c-71b3-4e6d-a8ae-aa23d264ae69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-03-03 08:58:41--  https://raw.githubusercontent.com/ChristinaROK/PreOnboarding_AI_assets/main/data/sample_df_test.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 101383 (99K) [text/plain]\n","Saving to: ‘sample_df_test.csv.6’\n","\n","sample_df_test.csv. 100%[===================>]  99.01K  --.-KB/s    in 0.007s  \n","\n","2022-03-03 08:58:41 (13.9 MB/s) - ‘sample_df_test.csv.6’ saved [101383/101383]\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:53.037044Z","start_time":"2022-02-02T04:01:52.707669Z"},"id":"KVo5dPnmUFxK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646298322138,"user_tz":-540,"elapsed":746,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"7a670fb5-9818-4f36-b75a-7a91faac8081"},"outputs":[{"output_type":"stream","name":"stdout","text":["train shape : (10000, 3)\n","test shape : (1000, 3)\n"]}],"source":["# 학습 & 평가 데이터셋 로드\n","# 학습 및 평가 샘플 데이터 개수는 각각 10,000개, 1,000개\n","\n","df_train = pd.read_csv('sample_df.csv')\n","df_test = pd.read_csv('sample_df_test.csv')\n","\n","print(f\"train shape : {df_train.shape}\")\n","print(f\"test shape : {df_test.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:53.085720Z","start_time":"2022-02-02T04:01:53.081413Z"},"id":"Ql82Ew2VUFxM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646298322500,"user_tz":-540,"elapsed":3,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"2439647e-0991-491a-d1c5-7fd3cb893327"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Dataset len: 10000\n","Train Dataset 1st element: ('나 이거 더빙을 누가하는지 모르고 봤는데 왠지 더빙이 구리더라...더빙이 너무 별로였음.', 0)\n","Test Dataset len: 1000\n","Test Dataset 1st element: ('신용문객잔 보고 후속편인줄 알고 봤더만 완전 개판이네 18.. 이련결 그냥 절에나 쳐 들어 가라.. 회오리에서 싸우는 신 참 가관이더라 .. 서극도 완전 쓰레기 감독이 다 됐구나.. 액션도 쓰레기고 배우들 연기도 참 가관이더라 18', 0)\n"]}],"source":["# Dataset 구현\n","# helper.py에 있는 CustomDataset 활용하여 train datset, test dataset 만들기\n","\n","train_dataset = CustomDataset(df_train.document, df_train.label)\n","test_dataset = CustomDataset(df_test.document, df_test.label)\n","\n","print(f\"Train Dataset len: {len(train_dataset)}\")\n","print(f\"Train Dataset 1st element: {train_dataset[0]}\")\n","\n","print(f\"Test Dataset len: {len(test_dataset)}\")\n","print(f\"Test Dataset 1st element: {test_dataset[0]}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:53.152070Z","start_time":"2022-02-02T04:01:53.145410Z"},"id":"7WUY6h8WUFxM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646298331496,"user_tz":-540,"elapsed":354,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"2483a56d-444d-4116-f1f7-811726046ad6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataset len: 9000\n","Valid dataset len: 1000\n"]}],"source":["# Train Dataset을 학습과 검증 셋으로 분리\n","# 학습 셋과 검증 셋의 비율은 9:1\n","# torch.utils.data에서 제공되는 데이터 세트를 임의로 분할할 수 있는 함수 찾아서 사용\n","n_train_sample = df_train.shape[0]\n","\n","n_train = int(n_train_sample*0.9)\n","n_valid = n_train_sample - n_train \n","train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [n_train, n_valid])\n","\n","print(f\"Train dataset len: {len(train_dataset)}\")\n","print(f\"Valid dataset len: {len(valid_dataset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:53.268838Z","start_time":"2022-02-02T04:01:53.263780Z"},"id":"H5nc7SpTUFxM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646298408684,"user_tz":-540,"elapsed":9,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"1a1390d2-4362-4210-e0ae-4cc218b04c82"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataloader # steps: 282\n","Valid dataloader # steps: 16\n","Test dataloader # steps: 16\n"]}],"source":["# DataLoader 구현\n","# train과 validation의 batch size는 각각 32, 64로 설정\n","# test의 batch size는 validation과 동일\n","# train에 사용할 DataLoader에서는 sampler로 RandomSampler 사용\n","# validation과 test에 사용할 DataLoader에서는 sampler로 SequentialSampler 사용\n","# 모든 DataLoader의 collate_fn은 helper.py에 있는 custom_collate_fn 사용\n","\n","train_batch_size = 32\n","valid_batch_size = 64\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, collate_fn=custom_collate_fn, sampler=RandomSampler(train_dataset))\n","\n","valid_dataloader = DataLoader(valid_dataset, batch_size=valid_batch_size, collate_fn=custom_collate_fn, sampler=SequentialSampler(valid_dataset))\n","\n","test_dataloader = DataLoader(test_dataset, batch_size=valid_batch_size, collate_fn=custom_collate_fn, sampler=SequentialSampler(test_dataset))\n","\n","print(f\"Train dataloader # steps: {len(train_dataloader)}\")\n","print(f\"Valid dataloader # steps: {len(valid_dataloader)}\")\n","print(f\"Test dataloader # steps: {len(test_dataloader)}\")"]},{"cell_type":"markdown","metadata":{"id":"9kEgqvBIUFxN"},"source":["### `auto_grad` 개념 복습\n","- torch의 `auto_grad` 기능\n","    - pytorch는 `requires_grad` 파리미터의 값이 True인 텐서에 한해서 미분값을 자동으로 계산한다.\n","    - 미분값은 `loss.backward()` 가 호출될 때 자동으로 계산된다."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-01-31T13:45:23.502936Z","start_time":"2022-01-31T13:45:20.029987Z"},"id":"oYjYpQ1DUFxN","colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["98d18fb90f924fb8bde62dbf839523c1","cd849fb46b594a19ae9230f7d0b132eb","5b77420dd0ec42d79f7ac0226e61a10e","5d60ced1c1e14e43b2bdbf5ae86916a9","b02bde81162b40f59107d778b99c9ec1","506df88525cb4350994377e316f16179","87f53651a97d426aab43a6d65ec1d26f","e0507c63a49d4e79a1779c562da629db","3e57aabdcfc94eaf972fc353c2f3a0d2","3b50bdbd106547b9a26ae2078ff8f2c7","01c4b032116b49c6adfb6d9d5446f15d","ceddce4bd1ae4940862fb58f960eb992","cfe6f1b158d3475fad73ba1d756598d8","d3b8b1eaa08d4a509b7a3f9bad46c664","5334d72753194928ae2d0c3c28a9ac68","dc87f6a4063f4b46a476ceb8924614cb","63eee48e81724b86940c26f4127a1633","2f260a136abb4da3be1abf4c45f87562","1409bc87fc3c4e1199c10d8fa7a6662a","ed7b22fec1bf4fe88ab7be8521cb1986","6ec435164d604f77897e01e21eeaf5ab","b72ae773a7004edc88430f8f0b106ac5"]},"executionInfo":{"status":"ok","timestamp":1646297951409,"user_tz":-540,"elapsed":10955,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"52d8b3d7-1f70-4186-c36e-86638cd537c9"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98d18fb90f924fb8bde62dbf839523c1","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/425 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ceddce4bd1ae4940862fb58f960eb992","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/424M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["# helper.py에 있는 CustomClassifier 모델을 로드해 model_freeze 변수에 instance를 생성\n","# hidden_size=768\n","# n_label=2\n","# freeze_base=True\n","\n","model_freeze = CustomClassifier(hidden_size=768, n_label=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-01-31T13:45:34.604914Z","start_time":"2022-01-31T13:45:34.586711Z"},"id":"XxNFh8KZUFxN"},"outputs":[],"source":["# model_freeze 모델의 모든 파라미터를 출력해보고 아래 질문에 답해 보자"]},{"cell_type":"code","source":["for _ in range(len(model_freeze.bert.encoder.layer)):\n","    print(f'BERT의 {_} layer 가중치')\n","    print(model_freeze.bert.encoder.layer[_].attention.self.query.weight)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZBGPYh6nErt","executionInfo":{"status":"ok","timestamp":1646296822600,"user_tz":-540,"elapsed":34,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"80c36e10-5d20-45f9-81ec-7cf2f0f25202"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BERT의 0 layer 가중치\n","Parameter containing:\n","tensor([[ 0.0136,  0.0162, -0.0616,  ..., -0.0227, -0.0551, -0.0385],\n","        [ 0.0549, -0.0462, -0.0229,  ...,  0.0028, -0.0088,  0.0135],\n","        [ 0.0275,  0.0846,  0.0093,  ..., -0.0018,  0.0184, -0.0297],\n","        ...,\n","        [ 0.0105, -0.0079, -0.0426,  ...,  0.0378, -0.0219,  0.0065],\n","        [-0.0283,  0.0635,  0.0240,  ...,  0.0183,  0.0244, -0.0108],\n","        [ 0.0601, -0.0081,  0.0419,  ...,  0.0085,  0.0259,  0.0199]],\n","       requires_grad=True)\n","\n","BERT의 1 layer 가중치\n","Parameter containing:\n","tensor([[ 0.0246,  0.0231,  0.0089,  ..., -0.0066,  0.0584,  0.0247],\n","        [ 0.0343, -0.0242, -0.0429,  ..., -0.0052, -0.0078,  0.0067],\n","        [ 0.0370, -0.0139,  0.0580,  ...,  0.0273, -0.0123, -0.0100],\n","        ...,\n","        [-0.0184, -0.0413, -0.0055,  ...,  0.0408, -0.0351, -0.0166],\n","        [ 0.0425, -0.0012, -0.0056,  ..., -0.0567, -0.0166,  0.0417],\n","        [-0.0009,  0.0309,  0.0646,  ...,  0.0216,  0.0070,  0.0309]],\n","       requires_grad=True)\n","\n","BERT의 2 layer 가중치\n","Parameter containing:\n","tensor([[ 0.0273, -0.0165,  0.0701,  ...,  0.0521, -0.0200, -0.0452],\n","        [ 0.0033,  0.0144,  0.0616,  ...,  0.0232,  0.0456,  0.0360],\n","        [ 0.0309, -0.0039,  0.0343,  ..., -0.0424, -0.0267, -0.0095],\n","        ...,\n","        [ 0.0566, -0.0047, -0.0654,  ..., -0.0283, -0.0206, -0.0285],\n","        [-0.0308,  0.0320,  0.0363,  ...,  0.0445, -0.0515, -0.0114],\n","        [-0.0247,  0.0246,  0.0090,  ...,  0.0070,  0.0347,  0.0213]],\n","       requires_grad=True)\n","\n","BERT의 3 layer 가중치\n","Parameter containing:\n","tensor([[-0.0212, -0.0799, -0.0067,  ..., -0.0276, -0.0343, -0.0292],\n","        [ 0.0181,  0.0238, -0.0244,  ..., -0.0327,  0.0585, -0.0406],\n","        [ 0.0296, -0.0290, -0.0096,  ..., -0.0232, -0.0956,  0.0353],\n","        ...,\n","        [-0.0193, -0.0022, -0.0096,  ...,  0.0427, -0.0459,  0.0090],\n","        [ 0.0113, -0.0443, -0.0219,  ...,  0.0193,  0.0258,  0.0225],\n","        [-0.0663, -0.0144, -0.0034,  ...,  0.0368,  0.0065, -0.0103]],\n","       requires_grad=True)\n","\n","BERT의 4 layer 가중치\n","Parameter containing:\n","tensor([[-0.0601,  0.0415,  0.0764,  ..., -0.0146,  0.0221,  0.0035],\n","        [-0.0029, -0.0347, -0.0012,  ...,  0.0022, -0.0248, -0.0073],\n","        [ 0.0287,  0.0232, -0.0379,  ...,  0.0001,  0.0092,  0.0104],\n","        ...,\n","        [-0.0149, -0.0002, -0.0580,  ...,  0.0597, -0.0278, -0.0550],\n","        [ 0.0482,  0.0096,  0.1111,  ...,  0.0417,  0.0169, -0.0431],\n","        [-0.0103, -0.0039,  0.0546,  ...,  0.0352,  0.0173, -0.0249]],\n","       requires_grad=True)\n","\n","BERT의 5 layer 가중치\n","Parameter containing:\n","tensor([[ 0.0286, -0.0249, -0.0405,  ...,  0.0385,  0.0507, -0.0067],\n","        [-0.0119, -0.0017, -0.0287,  ..., -0.0475, -0.0497,  0.0099],\n","        [-0.0356,  0.0113, -0.0346,  ...,  0.0233,  0.0568,  0.0588],\n","        ...,\n","        [ 0.0140,  0.0056,  0.0227,  ..., -0.0147, -0.0479,  0.0163],\n","        [-0.0020,  0.0112,  0.0828,  ...,  0.0078, -0.0571, -0.0256],\n","        [-0.0064, -0.0032,  0.0112,  ..., -0.0210, -0.0222, -0.0052]],\n","       requires_grad=True)\n","\n","BERT의 6 layer 가중치\n","Parameter containing:\n","tensor([[-0.0189, -0.0239, -0.0641,  ...,  0.0240,  0.0571,  0.0944],\n","        [-0.0233,  0.0113, -0.0987,  ...,  0.0018, -0.0219,  0.0130],\n","        [ 0.0912,  0.0087, -0.0120,  ...,  0.0096,  0.0193,  0.0461],\n","        ...,\n","        [ 0.0688,  0.0447, -0.0573,  ...,  0.0496, -0.0058,  0.0352],\n","        [ 0.0015,  0.0592, -0.0131,  ...,  0.0068,  0.0464,  0.0279],\n","        [ 0.0576, -0.0295,  0.1437,  ..., -0.0064, -0.0716,  0.0174]],\n","       requires_grad=True)\n","\n","BERT의 7 layer 가중치\n","Parameter containing:\n","tensor([[-0.0299, -0.0730,  0.0369,  ...,  0.0176,  0.0134,  0.0342],\n","        [ 0.0580,  0.0204, -0.0089,  ..., -0.0064, -0.0118, -0.0055],\n","        [ 0.0316,  0.0664,  0.0480,  ...,  0.0257, -0.0511, -0.0167],\n","        ...,\n","        [-0.0318,  0.0109, -0.0015,  ..., -0.0607,  0.0341,  0.0271],\n","        [ 0.0586,  0.0571,  0.0558,  ..., -0.0527, -0.0099, -0.0302],\n","        [-0.0043,  0.0106, -0.0273,  ..., -0.0491, -0.0435, -0.0029]],\n","       requires_grad=True)\n","\n","BERT의 8 layer 가중치\n","Parameter containing:\n","tensor([[ 0.0348, -0.0367, -0.0284,  ..., -0.0382,  0.0528, -0.0435],\n","        [ 0.0305, -0.0173, -0.0330,  ..., -0.0471, -0.0347,  0.0034],\n","        [-0.0123,  0.0496, -0.0677,  ..., -0.0043, -0.0839, -0.0442],\n","        ...,\n","        [ 0.0205,  0.0224,  0.0064,  ..., -0.0436, -0.0182,  0.0294],\n","        [ 0.0441, -0.0048,  0.0066,  ...,  0.0392, -0.0264, -0.0283],\n","        [-0.0487,  0.0169,  0.0484,  ..., -0.0137, -0.0327,  0.0525]],\n","       requires_grad=True)\n","\n","BERT의 9 layer 가중치\n","Parameter containing:\n","tensor([[-0.0009, -0.0528,  0.0472,  ..., -0.0366, -0.0259, -0.0370],\n","        [ 0.0330, -0.0278,  0.0779,  ...,  0.0661,  0.0227, -0.0182],\n","        [ 0.0479, -0.0608,  0.0140,  ..., -0.0011, -0.0572,  0.0243],\n","        ...,\n","        [-0.0054,  0.0240,  0.0452,  ..., -0.0375, -0.0427,  0.0161],\n","        [-0.0593,  0.0448,  0.0217,  ..., -0.0199, -0.0329,  0.0279],\n","        [-0.0280,  0.0437, -0.0365,  ...,  0.0120,  0.0350, -0.0332]],\n","       requires_grad=True)\n","\n","BERT의 10 layer 가중치\n","Parameter containing:\n","tensor([[-0.0235,  0.0747, -0.0575,  ..., -0.0141,  0.0151,  0.0090],\n","        [ 0.0525, -0.0145,  0.0009,  ..., -0.0012,  0.0383,  0.0534],\n","        [ 0.0038, -0.0198,  0.0134,  ..., -0.0363,  0.0524, -0.0076],\n","        ...,\n","        [ 0.0085, -0.0098, -0.0769,  ...,  0.0454,  0.0657, -0.0026],\n","        [ 0.0289,  0.0570,  0.0357,  ...,  0.0298, -0.0075, -0.0248],\n","        [ 0.0512,  0.0376, -0.0491,  ...,  0.0127, -0.0344, -0.1027]],\n","       requires_grad=True)\n","\n","BERT의 11 layer 가중치\n","Parameter containing:\n","tensor([[ 0.0493,  0.0184, -0.0275,  ..., -0.0241,  0.0096, -0.0127],\n","        [ 0.0441, -0.0675,  0.0111,  ..., -0.0492, -0.0162, -0.0204],\n","        [-0.0765,  0.0104,  0.0302,  ...,  0.0566, -0.0373,  0.0307],\n","        ...,\n","        [ 0.0244,  0.0093, -0.0188,  ..., -0.0117, -0.0706, -0.0201],\n","        [ 0.0129, -0.0276,  0.0755,  ...,  0.0056, -0.0247, -0.0041],\n","        [ 0.0025, -0.0537, -0.0403,  ...,  0.0702, -0.0359,  0.0658]],\n","       requires_grad=True)\n","\n"]}]},{"cell_type":"code","source":["for _ in range(len(model_freeze.classifier)):\n","    if type (model_freeze.classifier[_]) == torch.nn.modules.linear.Linear:\n","        print(f'Classifier의 {_} layer 가중치')\n","        print(f'{model_freeze.classifier[_].weight}')\n","        print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C1T8lwgvnd8x","executionInfo":{"status":"ok","timestamp":1646296822601,"user_tz":-540,"elapsed":32,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"d2beec03-e482-49b2-857a-09ff89223d59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Classifier의 0 layer 가중치\n","Parameter containing:\n","tensor([[-0.0175, -0.0060, -0.0251,  ...,  0.0293, -0.0258, -0.0341],\n","        [ 0.0007, -0.0295, -0.0096,  ..., -0.0269,  0.0298,  0.0083],\n","        [ 0.0201,  0.0189, -0.0330,  ..., -0.0082,  0.0135,  0.0053],\n","        ...,\n","        [ 0.0178,  0.0220,  0.0258,  ...,  0.0003,  0.0011,  0.0074],\n","        [ 0.0020, -0.0170, -0.0345,  ..., -0.0314, -0.0292, -0.0003],\n","        [-0.0113, -0.0169, -0.0195,  ..., -0.0068,  0.0018,  0.0051]],\n","       requires_grad=True)\n","\n","Classifier의 3 layer 가중치\n","Parameter containing:\n","tensor([[-0.0937,  0.0246, -0.0361, -0.1719,  0.1559, -0.1365,  0.0912,  0.1579,\n","          0.0589,  0.0352, -0.0070,  0.0884, -0.1504,  0.1570,  0.0769,  0.1590,\n","         -0.1125, -0.0891,  0.0533,  0.1434, -0.1295,  0.1195,  0.0360, -0.1480,\n","          0.1445, -0.1223, -0.0791,  0.0661,  0.1574, -0.1315,  0.0458,  0.0251],\n","        [ 0.1164, -0.1618,  0.0258,  0.1033,  0.0245,  0.0298,  0.0579,  0.0991,\n","         -0.0795,  0.1644,  0.1157,  0.0879,  0.0970, -0.1485,  0.1605,  0.1682,\n","          0.0367, -0.1154, -0.1474,  0.1604,  0.1322,  0.0170, -0.0614, -0.1460,\n","          0.0481, -0.0653,  0.0367, -0.1174, -0.1041, -0.0263,  0.1009, -0.0124]],\n","       requires_grad=True)\n","\n"]}]},{"cell_type":"code","source":["print(model_freeze.classifier[0].weight.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2muQRDvelzus","executionInfo":{"status":"ok","timestamp":1646296822601,"user_tz":-540,"elapsed":25,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"e8bc665c-c196-43bb-eb6b-6d9a14a87a44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 768])\n"]}]},{"cell_type":"code","source":["print(model_freeze.classifier[0].weight)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K6dtQ4xGl7G-","executionInfo":{"status":"ok","timestamp":1646296822601,"user_tz":-540,"elapsed":21,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"4853a069-7741-4351-cd15-58c548cecfeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[-0.0175, -0.0060, -0.0251,  ...,  0.0293, -0.0258, -0.0341],\n","        [ 0.0007, -0.0295, -0.0096,  ..., -0.0269,  0.0298,  0.0083],\n","        [ 0.0201,  0.0189, -0.0330,  ..., -0.0082,  0.0135,  0.0053],\n","        ...,\n","        [ 0.0178,  0.0220,  0.0258,  ...,  0.0003,  0.0011,  0.0074],\n","        [ 0.0020, -0.0170, -0.0345,  ..., -0.0314, -0.0292, -0.0003],\n","        [-0.0113, -0.0169, -0.0195,  ..., -0.0068,  0.0018,  0.0051]],\n","       requires_grad=True)\n"]}]},{"cell_type":"code","source":["print(model_freeze.classifier[0].weight.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YPrkPDXimCrl","executionInfo":{"status":"ok","timestamp":1646296822601,"user_tz":-540,"elapsed":18,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"1bae4cc5-34fb-4c75-d22e-ea7fb65675af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}]},{"cell_type":"markdown","metadata":{"id":"KloNNAKI5Q3r"},"source":["### `auto_grad` 개념 및 모델 구조 복습을 위해 다음 항목에 답해 보자\n","- `bert.encoder.layer.0.attention.self.query.weight` 텐서의 gradient는 True인 상태인가?\n","> True상태임\n","- `classifier.0.weight` 텐서의 shape은? \n","> 32 by 768\n","- `classifier.0.weight` 텐서는 freeze 상태인가 ? \n","> 아님. requires_grad가 True이므로 갱신될 수 있음.\n","- `classifier.0.weight` 텐서의 gradient 값은 무엇인가? \n","> None"]},{"cell_type":"markdown","metadata":{"id":"4iIrHg1xUFxP"},"source":["### 위 모델 (`model_freeze`)의 모든 파라미터의 gradient를 freeze 해보자"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-01-31T13:49:26.820569Z","start_time":"2022-01-31T13:49:26.816511Z"},"id":"sHkaFgC8UFxP"},"outputs":[],"source":["# 모든 파라미터의 gradient를 freeze 해보고 제대로 변경되었는지 \b확인하기 위해 모델의 모든 파라미터를 출력해보자."]},{"cell_type":"code","source":["for _ in range(len(model_freeze.bert.encoder.layer)):\n","    model_freeze.bert.encoder.layer[_].attention.self.query.weight.requires_grad = False\n","for _ in range(len(model_freeze.classifier)):\n","    if type (model_freeze.classifier[_]) == torch.nn.modules.linear.Linear:\n","        model_freeze.classifier[_].weight.requires_grad = False"],"metadata":{"id":"jCI3IyE1okdh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _ in range(len(model_freeze.bert.encoder.layer)):\n","    print(f'BERT의 {_} layer 가중치')\n","    print(model_freeze.bert.encoder.layer[_].attention.self.query.weight)\n","    print()\n","for _ in range(len(model_freeze.classifier)):\n","    if type (model_freeze.classifier[_]) == torch.nn.modules.linear.Linear:\n","        print(f'Classifier의 {_} layer 가중치')\n","        print(f'{model_freeze.classifier[_].weight}')\n","        print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646296822990,"user_tz":-540,"elapsed":4,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"d10eeda5-a779-49d4-d16c-63c501194923","id":"6Hwqv3WxoVSh"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["BERT의 0 layer 가중치\n","Parameter containing:\n","tensor([[ 0.0136,  0.0162, -0.0616,  ..., -0.0227, -0.0551, -0.0385],\n","        [ 0.0549, -0.0462, -0.0229,  ...,  0.0028, -0.0088,  0.0135],\n","        [ 0.0275,  0.0846,  0.0093,  ..., -0.0018,  0.0184, -0.0297],\n","        ...,\n","        [ 0.0105, -0.0079, -0.0426,  ...,  0.0378, -0.0219,  0.0065],\n","        [-0.0283,  0.0635,  0.0240,  ...,  0.0183,  0.0244, -0.0108],\n","        [ 0.0601, -0.0081,  0.0419,  ...,  0.0085,  0.0259,  0.0199]])\n","\n","BERT의 1 layer 가중치\n","Parameter containing:\n","tensor([[ 0.0246,  0.0231,  0.0089,  ..., -0.0066,  0.0584,  0.0247],\n","        [ 0.0343, -0.0242, -0.0429,  ..., -0.0052, -0.0078,  0.0067],\n","        [ 0.0370, -0.0139,  0.0580,  ...,  0.0273, -0.0123, -0.0100],\n","        ...,\n","        [-0.0184, -0.0413, -0.0055,  ...,  0.0408, -0.0351, -0.0166],\n","        [ 0.0425, -0.0012, -0.0056,  ..., -0.0567, -0.0166,  0.0417],\n","        [-0.0009,  0.0309,  0.0646,  ...,  0.0216,  0.0070,  0.0309]])\n","\n","BERT의 2 layer 가중치\n","Parameter containing:\n","tensor([[ 0.0273, -0.0165,  0.0701,  ...,  0.0521, -0.0200, -0.0452],\n","        [ 0.0033,  0.0144,  0.0616,  ...,  0.0232,  0.0456,  0.0360],\n","        [ 0.0309, -0.0039,  0.0343,  ..., -0.0424, -0.0267, -0.0095],\n","        ...,\n","        [ 0.0566, -0.0047, -0.0654,  ..., -0.0283, -0.0206, -0.0285],\n","        [-0.0308,  0.0320,  0.0363,  ...,  0.0445, -0.0515, -0.0114],\n","        [-0.0247,  0.0246,  0.0090,  ...,  0.0070,  0.0347,  0.0213]])\n","\n","BERT의 3 layer 가중치\n","Parameter containing:\n","tensor([[-0.0212, -0.0799, -0.0067,  ..., -0.0276, -0.0343, -0.0292],\n","        [ 0.0181,  0.0238, -0.0244,  ..., -0.0327,  0.0585, -0.0406],\n","        [ 0.0296, -0.0290, -0.0096,  ..., -0.0232, -0.0956,  0.0353],\n","        ...,\n","        [-0.0193, -0.0022, -0.0096,  ...,  0.0427, -0.0459,  0.0090],\n","        [ 0.0113, -0.0443, -0.0219,  ...,  0.0193,  0.0258,  0.0225],\n","        [-0.0663, -0.0144, -0.0034,  ...,  0.0368,  0.0065, -0.0103]])\n","\n","BERT의 4 layer 가중치\n","Parameter containing:\n","tensor([[-0.0601,  0.0415,  0.0764,  ..., -0.0146,  0.0221,  0.0035],\n","        [-0.0029, -0.0347, -0.0012,  ...,  0.0022, -0.0248, -0.0073],\n","        [ 0.0287,  0.0232, -0.0379,  ...,  0.0001,  0.0092,  0.0104],\n","        ...,\n","        [-0.0149, -0.0002, -0.0580,  ...,  0.0597, -0.0278, -0.0550],\n","        [ 0.0482,  0.0096,  0.1111,  ...,  0.0417,  0.0169, -0.0431],\n","        [-0.0103, -0.0039,  0.0546,  ...,  0.0352,  0.0173, -0.0249]])\n","\n","BERT의 5 layer 가중치\n","Parameter containing:\n","tensor([[ 0.0286, -0.0249, -0.0405,  ...,  0.0385,  0.0507, -0.0067],\n","        [-0.0119, -0.0017, -0.0287,  ..., -0.0475, -0.0497,  0.0099],\n","        [-0.0356,  0.0113, -0.0346,  ...,  0.0233,  0.0568,  0.0588],\n","        ...,\n","        [ 0.0140,  0.0056,  0.0227,  ..., -0.0147, -0.0479,  0.0163],\n","        [-0.0020,  0.0112,  0.0828,  ...,  0.0078, -0.0571, -0.0256],\n","        [-0.0064, -0.0032,  0.0112,  ..., -0.0210, -0.0222, -0.0052]])\n","\n","BERT의 6 layer 가중치\n","Parameter containing:\n","tensor([[-0.0189, -0.0239, -0.0641,  ...,  0.0240,  0.0571,  0.0944],\n","        [-0.0233,  0.0113, -0.0987,  ...,  0.0018, -0.0219,  0.0130],\n","        [ 0.0912,  0.0087, -0.0120,  ...,  0.0096,  0.0193,  0.0461],\n","        ...,\n","        [ 0.0688,  0.0447, -0.0573,  ...,  0.0496, -0.0058,  0.0352],\n","        [ 0.0015,  0.0592, -0.0131,  ...,  0.0068,  0.0464,  0.0279],\n","        [ 0.0576, -0.0295,  0.1437,  ..., -0.0064, -0.0716,  0.0174]])\n","\n","BERT의 7 layer 가중치\n","Parameter containing:\n","tensor([[-0.0299, -0.0730,  0.0369,  ...,  0.0176,  0.0134,  0.0342],\n","        [ 0.0580,  0.0204, -0.0089,  ..., -0.0064, -0.0118, -0.0055],\n","        [ 0.0316,  0.0664,  0.0480,  ...,  0.0257, -0.0511, -0.0167],\n","        ...,\n","        [-0.0318,  0.0109, -0.0015,  ..., -0.0607,  0.0341,  0.0271],\n","        [ 0.0586,  0.0571,  0.0558,  ..., -0.0527, -0.0099, -0.0302],\n","        [-0.0043,  0.0106, -0.0273,  ..., -0.0491, -0.0435, -0.0029]])\n","\n","BERT의 8 layer 가중치\n","Parameter containing:\n","tensor([[ 0.0348, -0.0367, -0.0284,  ..., -0.0382,  0.0528, -0.0435],\n","        [ 0.0305, -0.0173, -0.0330,  ..., -0.0471, -0.0347,  0.0034],\n","        [-0.0123,  0.0496, -0.0677,  ..., -0.0043, -0.0839, -0.0442],\n","        ...,\n","        [ 0.0205,  0.0224,  0.0064,  ..., -0.0436, -0.0182,  0.0294],\n","        [ 0.0441, -0.0048,  0.0066,  ...,  0.0392, -0.0264, -0.0283],\n","        [-0.0487,  0.0169,  0.0484,  ..., -0.0137, -0.0327,  0.0525]])\n","\n","BERT의 9 layer 가중치\n","Parameter containing:\n","tensor([[-0.0009, -0.0528,  0.0472,  ..., -0.0366, -0.0259, -0.0370],\n","        [ 0.0330, -0.0278,  0.0779,  ...,  0.0661,  0.0227, -0.0182],\n","        [ 0.0479, -0.0608,  0.0140,  ..., -0.0011, -0.0572,  0.0243],\n","        ...,\n","        [-0.0054,  0.0240,  0.0452,  ..., -0.0375, -0.0427,  0.0161],\n","        [-0.0593,  0.0448,  0.0217,  ..., -0.0199, -0.0329,  0.0279],\n","        [-0.0280,  0.0437, -0.0365,  ...,  0.0120,  0.0350, -0.0332]])\n","\n","BERT의 10 layer 가중치\n","Parameter containing:\n","tensor([[-0.0235,  0.0747, -0.0575,  ..., -0.0141,  0.0151,  0.0090],\n","        [ 0.0525, -0.0145,  0.0009,  ..., -0.0012,  0.0383,  0.0534],\n","        [ 0.0038, -0.0198,  0.0134,  ..., -0.0363,  0.0524, -0.0076],\n","        ...,\n","        [ 0.0085, -0.0098, -0.0769,  ...,  0.0454,  0.0657, -0.0026],\n","        [ 0.0289,  0.0570,  0.0357,  ...,  0.0298, -0.0075, -0.0248],\n","        [ 0.0512,  0.0376, -0.0491,  ...,  0.0127, -0.0344, -0.1027]])\n","\n","BERT의 11 layer 가중치\n","Parameter containing:\n","tensor([[ 0.0493,  0.0184, -0.0275,  ..., -0.0241,  0.0096, -0.0127],\n","        [ 0.0441, -0.0675,  0.0111,  ..., -0.0492, -0.0162, -0.0204],\n","        [-0.0765,  0.0104,  0.0302,  ...,  0.0566, -0.0373,  0.0307],\n","        ...,\n","        [ 0.0244,  0.0093, -0.0188,  ..., -0.0117, -0.0706, -0.0201],\n","        [ 0.0129, -0.0276,  0.0755,  ...,  0.0056, -0.0247, -0.0041],\n","        [ 0.0025, -0.0537, -0.0403,  ...,  0.0702, -0.0359,  0.0658]])\n","\n","Classifier의 0 layer 가중치\n","Parameter containing:\n","tensor([[-0.0175, -0.0060, -0.0251,  ...,  0.0293, -0.0258, -0.0341],\n","        [ 0.0007, -0.0295, -0.0096,  ..., -0.0269,  0.0298,  0.0083],\n","        [ 0.0201,  0.0189, -0.0330,  ..., -0.0082,  0.0135,  0.0053],\n","        ...,\n","        [ 0.0178,  0.0220,  0.0258,  ...,  0.0003,  0.0011,  0.0074],\n","        [ 0.0020, -0.0170, -0.0345,  ..., -0.0314, -0.0292, -0.0003],\n","        [-0.0113, -0.0169, -0.0195,  ..., -0.0068,  0.0018,  0.0051]])\n","\n","Classifier의 3 layer 가중치\n","Parameter containing:\n","tensor([[-0.0937,  0.0246, -0.0361, -0.1719,  0.1559, -0.1365,  0.0912,  0.1579,\n","          0.0589,  0.0352, -0.0070,  0.0884, -0.1504,  0.1570,  0.0769,  0.1590,\n","         -0.1125, -0.0891,  0.0533,  0.1434, -0.1295,  0.1195,  0.0360, -0.1480,\n","          0.1445, -0.1223, -0.0791,  0.0661,  0.1574, -0.1315,  0.0458,  0.0251],\n","        [ 0.1164, -0.1618,  0.0258,  0.1033,  0.0245,  0.0298,  0.0579,  0.0991,\n","         -0.0795,  0.1644,  0.1157,  0.0879,  0.0970, -0.1485,  0.1605,  0.1682,\n","          0.0367, -0.1154, -0.1474,  0.1604,  0.1322,  0.0170, -0.0614, -0.1460,\n","          0.0481, -0.0653,  0.0367, -0.1174, -0.1041, -0.0263,  0.1009, -0.0124]])\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"NsMgM3sK5Q3t"},"source":["## Challenge"]},{"cell_type":"markdown","metadata":{"id":"vUn-6PFP5Q3t"},"source":["### `scheduler` 를 생성 \n","- 스케쥴러를 알기 전에 먼저 `epoch`의 개념을 알아야 한다. Epoch는 dataset를 **몇 번 반복**해 학습할 것인지를 의미한다. 만약 dataset의 개수가 2,000개이고 epoch을 2번 학습하게 되면 총 4,000개의 데이터를 학습하게 된다.   \n","- 스케쥴러는 epoch에 따라 learning rate의 값을 조정하는 것을 의미한다. \n","- 예를 들어 [여기](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup)의 그림에서 볼 수 있듯이 `get_linear_schedule_with_warmup`는 특정 step까지는 learning rate를 천천히 상승시키다가 고점에 도달하면 다시 하락시킨다. "]},{"cell_type":"markdown","metadata":{"id":"_FuADvuT5Q3t"},"source":["### `model`, `optimizer`, `scheduler`를 초기화(=인스턴스 생성)하는 함수를 구현하라"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:59.217735Z","start_time":"2022-02-02T04:01:59.210482Z"},"id":"-sE7xjYcRD1p"},"outputs":[],"source":["from torch.nn import CrossEntropyLoss\n","from torch.optim import AdamW\n","from torch.nn.utils import clip_grad_norm_\n","from transformers import get_linear_schedule_with_warmup, get_constant_schedule"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:01:59.549660Z","start_time":"2022-02-02T04:01:59.545752Z"},"id":"2eTFXzy8VK9R"},"outputs":[],"source":["# model:CustomClassifier 사용, hidden size는 768, label 개수는 2\n","# optimizer: AdamW 사용, learning rate는 2e-5\n","# scheduler: transformers.get_linear_schedule_with_warmup 함수 사용, 단, num_warmup_steps 매개 변수는 사용하지 않음\n","\n","def initializer(train_dataloader, epochs=2):\n","    \"\"\"\n","    모델, 옵티마이저, 스케쥴러를 초기화한 후 반환\n","    \"\"\"\n","    \n","    model = CustomClassifier(hidden_size=768, n_label=2)\n","\n","    optimizer = AdamW(model.parameters(),lr=1e-5)\n","    \n","    total_steps = len(train_dataloader) * epochs\n","    print(f\"Total train steps with {epochs} epochs: {total_steps}\")\n","\n","    scheduler = get_linear_schedule_with_warmup(\n","                                                optimizer, \n","                                                num_warmup_steps=0,\n","                                                num_training_steps=total_steps\n","                                                )\n","\n","    return model, optimizer, scheduler"]},{"cell_type":"markdown","metadata":{"id":"Xz-8_5as5Q3u"},"source":["### model, optimizer, scheduler의 파라미터 저장하는 함수를 구현하라"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:02:02.786877Z","start_time":"2022-02-02T04:02:02.783726Z"},"id":"vIP1BjFA5Q3u"},"outputs":[],"source":["# 모델 저장 함수 구현\n","\n","def save_checkpoint(path, model, optimizer, scheduler, epoch, loss):\n","    file_name = f'{path}/model.ckpt.{epoch}'\n","    \n","    # torch.save 함수 참고\n","    torch.save(\n","        {\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'loss' : loss\n","        }, \n","        file_name\n","    )\n","    \n","    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"]},{"cell_type":"markdown","metadata":{"id":"a3BUrgtJ5Q3v"},"source":["### `validate()` 함수 구현 \n","- `validate()` 함수 내 model의 상태는 **evaluate**이어야 한다. evaluate 상태의 model은 dropout을 진행하지 않는다. \n","- **forward**를 진행할 때 `with torch.no_grad(): ...` 설정해 미분 계산을 방지한다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:02:11.636684Z","start_time":"2022-02-02T04:02:11.631550Z"},"id":"VHpuV0CXUFxR"},"outputs":[],"source":["# input: model, valid_dataloader\n","# output: loss, 정확도\n","\n","def validate(model, valid_dataloader):\n","    global loss_fct\n","   \n","    # 모델을 evaluate 모드로 설정 & device 할당\n","    model.eval()\n","    \n","    total_loss, total_acc= 0,0\n","        \n","    for step, batch in enumerate(valid_dataloader):\n","        \n","        # tensor 연산 전, 각 tensor에 device 할당\n","        batch = tuple(item.to(device) for item in batch)\n","            \n","        batch_input, batch_label = batch[0], batch[1]\n","            \n","        # gradient 계산하지 않고 forward 진행\n","        with torch.no_grad():\n","            logits = model(**batch_input)\n","            \n","        # loss\n","        loss = loss_fct(logits, batch_label)\n","        total_loss += loss.item()\n","        \n","        # accuracy\n","        probs = F.softmax(logits, dim=1)\n","        preds = torch.argmax(probs, dim=1).flatten()\n","        acc = (preds == batch_label).cpu().numpy().mean()\n","        total_acc+=acc\n","    \n","    total_loss = total_loss/(step+1)\n","    total_acc = total_acc/(step+1)*100\n","\n","    return total_loss, total_acc\n"]},{"cell_type":"markdown","metadata":{"id":"NukaJc15UFxQ"},"source":["### `train()` 함수에 `epoch`와 `clip_grad_norm` 추가\n","- data_loader를 `epoch`만큼 반복하면서 학습하도록 `train()` 함수를 수정하라\n","- `gradient cliping`은 미분 값 너무 큰 경우 gradient exploding되는 현상을 막기 위해 미분값이 `threshold`를 넘을 경우 특정 비율을 미분 값에 곱해 크기를 줄여준다.\n","- Reference\n","  - [clip_grad_norm_ official document](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n","  - [그래디언트 클립핑 설명 한국어 블로그](https://kh-kim.gitbook.io/natural-language-processing-with-pytorch/00-cover-6/05-gradient-clipping)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T04:02:10.624280Z","start_time":"2022-02-02T04:02:10.615781Z"},"id":"ZvY5rxDKHQAp"},"outputs":[],"source":["# 위에서 구현한 모델 저장 함수(save_checkpoint)와 validate 함수도 추가해보자\n","\n","loss_fct = CrossEntropyLoss()\n","\n","def train(model, train_dataloader, valid_dataloader=None, epochs=2):\n","    global scheduler, loss_fct\n","    \n","    # train_dataloaer 학습을 epochs만큼 반복\n","    for epoch in range(epochs):\n","        print(f\"*****Epoch {epoch} Train Start*****\")\n","        \n","        # 배치 단위 평균 loss와 총 평균 loss 계산하기위해 변수 생성\n","        total_loss, batch_loss, batch_count = 0,0,0\n","    \n","        # model을 train 모드로 설정 & device 할당\n","        model.train()\n","        model.to(device)\n","        \n","        # data iterator를 돌면서 하나씩 학습\n","        for step, batch in enumerate(train_dataloader):\n","            batch_count+=1\n","            \n","            # tensor 연산 전, 각 tensor에 device 할당\n","            batch = tuple(item.to(device) for item in batch)\n","        \n","            batch_input, batch_label = batch\n","        \n","            # batch마다 모델이 갖고 있는 기존 gradient를 초기화\n","            model.zero_grad()\n","        \n","            # forward\n","            logits = model(**batch_input)\n","        \n","            # loss\n","            loss = loss_fct(logits, batch_label)\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","        \n","            # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n","            loss.backward()\n","            \n","            # gradient clipping 적용 (max_norm = 1)\n","            clip_grad_norm_(\n","                            model.parameters(),\n","                            max_norm=1\n","                            )\n","            \n","            # optimizer & scheduler 업데이트\n","            optimizer.step()\n","            scheduler.step()\n","            \n","            # 배치 10개씩 처리할 때마다 평균 loss와 lr를 출력\n","            if (step % 10 == 0 and step != 0):\n","                learning_rate = optimizer.param_groups[0]['lr']\n","                print(f\"Epoch: {epoch}, Step : {step}, LR : {learning_rate}, Avg Loss : {batch_loss / batch_count:.4f}\")\n","\n","                # reset \n","                batch_loss, batch_count = 0,0\n","\n","        print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n","        print(f\"*****Epoch {epoch} Train Finish*****\\n\")\n","        \n","        if valid_dataloader is not None:\n","            print(f\"*****Epoch {epoch} Valid Start*****\")\n","            valid_loss, valid_acc = validate(model, valid_dataloader)\n","            print(f\"Epoch {epoch} Valid Loss : {valid_loss:.4f} Valid Acc : {valid_acc:.2f}\")\n","            print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n","        \n","        # checkpoint 저장\n","        save_checkpoint('checkpoint', model, optimizer, scheduler, epoch, total_loss)\n","            \n","    print(\"Train Completed. End Program.\")"]},{"cell_type":"markdown","metadata":{"id":"4NWKzxIaf1QJ"},"source":["## Advanced"]},{"cell_type":"markdown","metadata":{"id":"gFWnii7a5Q3w"},"source":["### 학습 데이터를 epoch 4까지 학습\n","- 매 epoch마다 다음을 수행한다.\n","  - 학습이 끝난 후 validate() 함수 실행 \n","  - validate() 함수가 끝난 후 model save 함수 실행"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:02:11.377612Z","start_time":"2022-02-02T04:02:20.931961Z"},"id":"7Er1qKtsf1QJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646304011188,"user_tz":-540,"elapsed":3596419,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"7147e835-1695-4e99-c5fc-7e0a7106f14a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Total train steps with 4 epochs: 1128\n","*****Epoch 0 Train Start*****\n","Epoch: 0, Step : 10, LR : 9.902482269503547e-06, Avg Loss : 0.6740\n","Epoch: 0, Step : 20, LR : 9.813829787234044e-06, Avg Loss : 0.6378\n","Epoch: 0, Step : 30, LR : 9.72517730496454e-06, Avg Loss : 0.5930\n","Epoch: 0, Step : 40, LR : 9.636524822695035e-06, Avg Loss : 0.5566\n","Epoch: 0, Step : 50, LR : 9.547872340425532e-06, Avg Loss : 0.5141\n","Epoch: 0, Step : 60, LR : 9.45921985815603e-06, Avg Loss : 0.4564\n","Epoch: 0, Step : 70, LR : 9.370567375886526e-06, Avg Loss : 0.4860\n","Epoch: 0, Step : 80, LR : 9.281914893617022e-06, Avg Loss : 0.4236\n","Epoch: 0, Step : 90, LR : 9.193262411347519e-06, Avg Loss : 0.4537\n","Epoch: 0, Step : 100, LR : 9.104609929078016e-06, Avg Loss : 0.3982\n","Epoch: 0, Step : 110, LR : 9.015957446808511e-06, Avg Loss : 0.4254\n","Epoch: 0, Step : 120, LR : 8.927304964539007e-06, Avg Loss : 0.4063\n","Epoch: 0, Step : 130, LR : 8.838652482269504e-06, Avg Loss : 0.4233\n","Epoch: 0, Step : 140, LR : 8.750000000000001e-06, Avg Loss : 0.3533\n","Epoch: 0, Step : 150, LR : 8.661347517730498e-06, Avg Loss : 0.3228\n","Epoch: 0, Step : 160, LR : 8.572695035460993e-06, Avg Loss : 0.3442\n","Epoch: 0, Step : 170, LR : 8.48404255319149e-06, Avg Loss : 0.4460\n","Epoch: 0, Step : 180, LR : 8.395390070921986e-06, Avg Loss : 0.4441\n","Epoch: 0, Step : 190, LR : 8.306737588652483e-06, Avg Loss : 0.3607\n","Epoch: 0, Step : 200, LR : 8.218085106382978e-06, Avg Loss : 0.3419\n","Epoch: 0, Step : 210, LR : 8.129432624113476e-06, Avg Loss : 0.4006\n","Epoch: 0, Step : 220, LR : 8.040780141843973e-06, Avg Loss : 0.4007\n","Epoch: 0, Step : 230, LR : 7.95212765957447e-06, Avg Loss : 0.3990\n","Epoch: 0, Step : 240, LR : 7.863475177304965e-06, Avg Loss : 0.3387\n","Epoch: 0, Step : 250, LR : 7.774822695035462e-06, Avg Loss : 0.3743\n","Epoch: 0, Step : 260, LR : 7.686170212765958e-06, Avg Loss : 0.3397\n","Epoch: 0, Step : 270, LR : 7.597517730496454e-06, Avg Loss : 0.3791\n","Epoch: 0, Step : 280, LR : 7.508865248226951e-06, Avg Loss : 0.3820\n","Epoch 0 Total Mean Loss : 0.4319\n","*****Epoch 0 Train Finish*****\n","\n","*****Epoch 0 Valid Start*****\n","Epoch 0 Valid Loss : 0.3748 Valid Acc : 83.38\n","*****Epoch 0 Valid Finish*****\n","\n","Saving epoch 0 checkpoint at checkpoint/model.ckpt.0\n","*****Epoch 1 Train Start*****\n","Epoch: 1, Step : 10, LR : 7.402482269503547e-06, Avg Loss : 0.3236\n","Epoch: 1, Step : 20, LR : 7.313829787234044e-06, Avg Loss : 0.3332\n","Epoch: 1, Step : 30, LR : 7.225177304964539e-06, Avg Loss : 0.2880\n","Epoch: 1, Step : 40, LR : 7.136524822695035e-06, Avg Loss : 0.2721\n","Epoch: 1, Step : 50, LR : 7.047872340425532e-06, Avg Loss : 0.2969\n","Epoch: 1, Step : 60, LR : 6.959219858156029e-06, Avg Loss : 0.3084\n","Epoch: 1, Step : 70, LR : 6.870567375886526e-06, Avg Loss : 0.2882\n","Epoch: 1, Step : 80, LR : 6.781914893617022e-06, Avg Loss : 0.2979\n","Epoch: 1, Step : 90, LR : 6.693262411347519e-06, Avg Loss : 0.2521\n","Epoch: 1, Step : 100, LR : 6.604609929078015e-06, Avg Loss : 0.2827\n","Epoch: 1, Step : 110, LR : 6.515957446808511e-06, Avg Loss : 0.3139\n","Epoch: 1, Step : 120, LR : 6.427304964539007e-06, Avg Loss : 0.2638\n","Epoch: 1, Step : 130, LR : 6.338652482269504e-06, Avg Loss : 0.3423\n","Epoch: 1, Step : 140, LR : 6.25e-06, Avg Loss : 0.2899\n","Epoch: 1, Step : 150, LR : 6.161347517730497e-06, Avg Loss : 0.2744\n","Epoch: 1, Step : 160, LR : 6.072695035460994e-06, Avg Loss : 0.2737\n","Epoch: 1, Step : 170, LR : 5.98404255319149e-06, Avg Loss : 0.3021\n","Epoch: 1, Step : 180, LR : 5.895390070921986e-06, Avg Loss : 0.2729\n","Epoch: 1, Step : 190, LR : 5.806737588652482e-06, Avg Loss : 0.2473\n","Epoch: 1, Step : 200, LR : 5.718085106382979e-06, Avg Loss : 0.2728\n","Epoch: 1, Step : 210, LR : 5.629432624113476e-06, Avg Loss : 0.3257\n","Epoch: 1, Step : 220, LR : 5.540780141843972e-06, Avg Loss : 0.3293\n","Epoch: 1, Step : 230, LR : 5.452127659574469e-06, Avg Loss : 0.3356\n","Epoch: 1, Step : 240, LR : 5.363475177304965e-06, Avg Loss : 0.2383\n","Epoch: 1, Step : 250, LR : 5.274822695035462e-06, Avg Loss : 0.2680\n","Epoch: 1, Step : 260, LR : 5.186170212765958e-06, Avg Loss : 0.2120\n","Epoch: 1, Step : 270, LR : 5.097517730496454e-06, Avg Loss : 0.3198\n","Epoch: 1, Step : 280, LR : 5.00886524822695e-06, Avg Loss : 0.2966\n","Epoch 1 Total Mean Loss : 0.2895\n","*****Epoch 1 Train Finish*****\n","\n","*****Epoch 1 Valid Start*****\n","Epoch 1 Valid Loss : 0.3638 Valid Acc : 84.92\n","*****Epoch 1 Valid Finish*****\n","\n","Saving epoch 1 checkpoint at checkpoint/model.ckpt.1\n","*****Epoch 2 Train Start*****\n","Epoch: 2, Step : 10, LR : 4.902482269503547e-06, Avg Loss : 0.2200\n","Epoch: 2, Step : 20, LR : 4.813829787234043e-06, Avg Loss : 0.1593\n","Epoch: 2, Step : 30, LR : 4.725177304964539e-06, Avg Loss : 0.2394\n","Epoch: 2, Step : 40, LR : 4.636524822695036e-06, Avg Loss : 0.2875\n","Epoch: 2, Step : 50, LR : 4.547872340425532e-06, Avg Loss : 0.2379\n","Epoch: 2, Step : 60, LR : 4.459219858156029e-06, Avg Loss : 0.1942\n","Epoch: 2, Step : 70, LR : 4.370567375886525e-06, Avg Loss : 0.2686\n","Epoch: 2, Step : 80, LR : 4.281914893617022e-06, Avg Loss : 0.2498\n","Epoch: 2, Step : 90, LR : 4.1932624113475176e-06, Avg Loss : 0.2153\n","Epoch: 2, Step : 100, LR : 4.104609929078015e-06, Avg Loss : 0.1873\n","Epoch: 2, Step : 110, LR : 4.015957446808511e-06, Avg Loss : 0.2090\n","Epoch: 2, Step : 120, LR : 3.927304964539008e-06, Avg Loss : 0.2082\n","Epoch: 2, Step : 130, LR : 3.838652482269503e-06, Avg Loss : 0.1995\n","Epoch: 2, Step : 140, LR : 3.7500000000000005e-06, Avg Loss : 0.2851\n","Epoch: 2, Step : 150, LR : 3.6613475177304968e-06, Avg Loss : 0.2056\n","Epoch: 2, Step : 160, LR : 3.572695035460993e-06, Avg Loss : 0.3146\n","Epoch: 2, Step : 170, LR : 3.4840425531914897e-06, Avg Loss : 0.2257\n","Epoch: 2, Step : 180, LR : 3.395390070921986e-06, Avg Loss : 0.2244\n","Epoch: 2, Step : 190, LR : 3.3067375886524826e-06, Avg Loss : 0.2449\n","Epoch: 2, Step : 200, LR : 3.218085106382979e-06, Avg Loss : 0.2087\n","Epoch: 2, Step : 210, LR : 3.1294326241134755e-06, Avg Loss : 0.1837\n","Epoch: 2, Step : 220, LR : 3.040780141843972e-06, Avg Loss : 0.2073\n","Epoch: 2, Step : 230, LR : 2.9521276595744685e-06, Avg Loss : 0.2231\n","Epoch: 2, Step : 240, LR : 2.8634751773049647e-06, Avg Loss : 0.2630\n","Epoch: 2, Step : 250, LR : 2.7748226950354614e-06, Avg Loss : 0.2262\n","Epoch: 2, Step : 260, LR : 2.6861702127659577e-06, Avg Loss : 0.2427\n","Epoch: 2, Step : 270, LR : 2.597517730496454e-06, Avg Loss : 0.2703\n","Epoch: 2, Step : 280, LR : 2.5088652482269506e-06, Avg Loss : 0.2110\n","Epoch 2 Total Mean Loss : 0.2292\n","*****Epoch 2 Train Finish*****\n","\n","*****Epoch 2 Valid Start*****\n","Epoch 2 Valid Loss : 0.3678 Valid Acc : 84.96\n","*****Epoch 2 Valid Finish*****\n","\n","Saving epoch 2 checkpoint at checkpoint/model.ckpt.2\n","*****Epoch 3 Train Start*****\n","Epoch: 3, Step : 10, LR : 2.4024822695035465e-06, Avg Loss : 0.1962\n","Epoch: 3, Step : 20, LR : 2.3138297872340428e-06, Avg Loss : 0.1816\n","Epoch: 3, Step : 30, LR : 2.225177304964539e-06, Avg Loss : 0.2362\n","Epoch: 3, Step : 40, LR : 2.1365248226950357e-06, Avg Loss : 0.1844\n","Epoch: 3, Step : 50, LR : 2.047872340425532e-06, Avg Loss : 0.1929\n","Epoch: 3, Step : 60, LR : 1.9592198581560286e-06, Avg Loss : 0.1498\n","Epoch: 3, Step : 70, LR : 1.870567375886525e-06, Avg Loss : 0.1975\n","Epoch: 3, Step : 80, LR : 1.7819148936170213e-06, Avg Loss : 0.2573\n","Epoch: 3, Step : 90, LR : 1.693262411347518e-06, Avg Loss : 0.1520\n","Epoch: 3, Step : 100, LR : 1.6046099290780142e-06, Avg Loss : 0.1637\n","Epoch: 3, Step : 110, LR : 1.515957446808511e-06, Avg Loss : 0.1758\n","Epoch: 3, Step : 120, LR : 1.4273049645390072e-06, Avg Loss : 0.1218\n","Epoch: 3, Step : 130, LR : 1.3386524822695039e-06, Avg Loss : 0.1888\n","Epoch: 3, Step : 140, LR : 1.25e-06, Avg Loss : 0.1923\n","Epoch: 3, Step : 150, LR : 1.1613475177304966e-06, Avg Loss : 0.2310\n","Epoch: 3, Step : 160, LR : 1.072695035460993e-06, Avg Loss : 0.1886\n","Epoch: 3, Step : 170, LR : 9.840425531914895e-07, Avg Loss : 0.1899\n","Epoch: 3, Step : 180, LR : 8.95390070921986e-07, Avg Loss : 0.1683\n","Epoch: 3, Step : 190, LR : 8.067375886524824e-07, Avg Loss : 0.1881\n","Epoch: 3, Step : 200, LR : 7.180851063829789e-07, Avg Loss : 0.2012\n","Epoch: 3, Step : 210, LR : 6.294326241134751e-07, Avg Loss : 0.1723\n","Epoch: 3, Step : 220, LR : 5.407801418439717e-07, Avg Loss : 0.1644\n","Epoch: 3, Step : 230, LR : 4.5212765957446816e-07, Avg Loss : 0.1767\n","Epoch: 3, Step : 240, LR : 3.634751773049645e-07, Avg Loss : 0.2104\n","Epoch: 3, Step : 250, LR : 2.7482269503546104e-07, Avg Loss : 0.2361\n","Epoch: 3, Step : 260, LR : 1.8617021276595745e-07, Avg Loss : 0.1935\n","Epoch: 3, Step : 270, LR : 9.751773049645391e-08, Avg Loss : 0.2308\n","Epoch: 3, Step : 280, LR : 8.865248226950355e-09, Avg Loss : 0.1919\n","Epoch 3 Total Mean Loss : 0.1916\n","*****Epoch 3 Train Finish*****\n","\n","*****Epoch 3 Valid Start*****\n","Epoch 3 Valid Loss : 0.3894 Valid Acc : 84.77\n","*****Epoch 3 Valid Finish*****\n","\n","Saving epoch 3 checkpoint at checkpoint/model.ckpt.3\n","Train Completed. End Program.\n"]}],"source":["# 4 epoch 학습\n","epochs=4\n","model, optimizer, scheduler = initializer(train_dataloader, epochs)\n","train(model, train_dataloader, valid_dataloader, epochs)"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2022-02-02T03:27:18.246441Z","start_time":"2022-02-02T03:27:18.236617Z"},"id":"vA3_vqqCXccc"},"source":["### 가장 dev acc 성능이 높았던 epoch의 모델의 체크 포인트를 불러와 로드하자"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:22:27.646150Z","start_time":"2022-02-02T06:22:26.945572Z"},"id":"mvfkSff25Q3z"},"outputs":[],"source":["# torch.load 함수 사용\n","checkpoint = torch.load('checkpoint/model.ckpt.2')"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:22:36.415665Z","start_time":"2022-02-02T06:22:36.407250Z"},"id":"YqcxMmTj5Q3z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646304655200,"user_tz":-540,"elapsed":471,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"44c507c9-a380-46d3-cd50-38401506be75"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'loss'])"]},"metadata":{},"execution_count":86}],"source":["# checkpoint의 key 종류를 확인\n","checkpoint.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:22:40.272939Z","start_time":"2022-02-02T06:22:37.010491Z"},"id":"wTvFYgNi5Q30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646304657793,"user_tz":-540,"elapsed":2022,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"28c25c92-a4c5-4e34-a0d8-1c64fccd2304"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Total train steps with 1 epochs: 282\n"]}],"source":["# 위에서 구현한 initializer 함수 사용하여 model, optimizer, scheduler 초기화\n","\n","epochs=1\n","model, optimizer, scheduler = initializer(train_dataloader, epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:22:40.443912Z","start_time":"2022-02-02T06:22:40.274323Z"},"id":"CtR2sTW55Q30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646304657799,"user_tz":-540,"elapsed":57,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"923ac936-d567-4b6c-effa-ecb035c193c9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":88}],"source":["model.load_state_dict(checkpoint[\"model_state_dict\"])"]},{"cell_type":"markdown","metadata":{"id":"Tzske7SR5Q30"},"source":["### 모델 예측 함수 구현\n","- test_dataloader를 입력받아 모델이 예측한 확률값 (probs)과 실제 정답 (label) 을 출력하는 `\bpredict()` 함수를 구현하자.\n","- 함수 정의\n","  - 입력 매개변수\n","    - `model` : `CustomClassifier` 모델. logits를 반환함 \n","    - `test_dataloader` : test 데이터셋의 텍스트와 레이블을 배치로 갖는 dataloader\n","  - 조건\n","    - `test_dataloader`는 이터레이터기 때문에 이터레이터를 순회하면서 `all_logits` 리스트에 배치 단위의 logits를 저장하고 `all_labels` 리스트에 배치 단위의 레이블 (0 또는 1 값)을 저장하라\n","  - 반환값\n","    - `probs`\n","      - logits에 softmax 함수를 취한 확률값. (test data 개수, label 개수) shape을 가짐. \bnp.array 타입으로 데이터 타입을 변환할 것.\n","    - `labels`\n","      - 0 또는 1 값을 갖는 np.array. (test data 개수,) shape을 가짐."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:22:48.062229Z","start_time":"2022-02-02T06:22:48.057531Z"},"id":"yQ7WiD1Oigg9"},"outputs":[],"source":["\n","def predict(model, test_dataloader):\n","    \"\"\"\n","    test_dataloader의 label별 확률값과 실제 label 값을 반환\n","    \"\"\"\n","\n","    # model을 eval 모드로 설정 & device 할당\n","    model.eval()\n","    model.to(device)\n","\n","    all_logits = []\n","    all_labels = []\n","\n","    for step, batch in enumerate(test_dataloader):\n","        \n","        batch_input, batch_label = batch\n","        \n","        # batch_input을 device 할당\n","        batch_input.to(device)\n","        with torch.no_grad():\n","            # model에 batch_input을 넣어 logit 반환 & all_logits, all_labels 리스트에 값 추가 \n","            logits = model(**batch_input)\n","\n","        all_logits.extend(torch.argmax(logits, dim=1).detach().cpu().tolist())\n","        all_labels.extend(batch_label)\n","\n","    probs = np.array(all_logits) # logits을 확률값으로 변환 & Tensor 타입을 numpy.array 타입으로 변환\n","    all_labels = np.array(all_labels) #  Tensor 타입을 numpy.array 타입으로 변환\n","\n","    return probs, all_labels\n","\n"]},{"cell_type":"markdown","source":["- 모델이 예측한 확률값과 실제 label을 입력 받아 정확도를 출력하는 **accuracy()** 함수를 구현하자. \n","- 함수 정의 \n","  - 입력 매개변수 \n","    - `probs` : `predict()` 함수의 반환값. 2차원의 np.array\n","    - `labels` : `predict()` 함수의 반환값. 1차원의 np.array\n","  - 조건\n","    - `probs`의 확률값이 0.5 이상이면 1, 이하이면 0이 되도록 만든다. 모델이 예측한 레이블을 실제값(`labels`)과 비교해 예측값과 실제값이 같으면 1, 다르면 0 점수를 준다. 모든 데이터에 대해 점수의 평균값이 accuracy 값이다. \n","  - 반환값 \n","    - `acc` : 정확도 (Float type)"],"metadata":{"id":"lOxCjZ2g6ZeK"}},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:22:48.296419Z","start_time":"2022-02-02T06:22:48.293737Z"},"id":"42-umZ3m5Q32"},"outputs":[],"source":["# accuracy 함수 구현\n","def accuracy(probs, labels):\n","    y_pred = probs # probs(확률값)을 label로 변경(0.5 이상이면 1, 0.5 미만이면 0)\n","    acc = (y_pred == labels).sum() / len(labels) # 정확도 계산\n","    return acc "]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:24:22.752497Z","start_time":"2022-02-02T06:22:48.652784Z"},"id":"SwkrRPAhjsXb"},"outputs":[],"source":["probs, labels = predict(model, test_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:24:22.759367Z","start_time":"2022-02-02T06:24:22.753997Z"},"id":"MxDI8PRA5Q32","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646305143416,"user_tz":-540,"elapsed":383,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"b93b093c-77bd-4ac7-e402-e6d99ce89729"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.864"]},"metadata":{},"execution_count":97}],"source":["accuracy(probs, labels)"]},{"cell_type":"markdown","metadata":{"id":"3mqUfkx-5Q33"},"source":["### `sklearn.metrics`의 `accuracy_score`, `roc_auc_score` 함수를 이용해 정확도와 auc를 계산하라"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:24:23.111879Z","start_time":"2022-02-02T06:24:22.760568Z"},"id":"VFWj4lcp5Q33"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score, accuracy_score"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:24:23.116872Z","start_time":"2022-02-02T06:24:23.113064Z"},"id":"p9BEe2mflTem","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646305146168,"user_tz":-540,"elapsed":15,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"85097059-2a87-4b53-9d7f-9f99fa6c8c7e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.864"]},"metadata":{},"execution_count":99}],"source":["# 정확도 출력\n","\n","accuracy_score(probs, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-02T06:24:23.125650Z","start_time":"2022-02-02T06:24:23.117847Z"},"id":"oCl6BiPGpCPW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646305146170,"user_tz":-540,"elapsed":14,"user":{"displayName":"KYEONGCHAN LEE","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijlkBQ-RF-9M31bBoivZTgflWV0Gg6YRg4O7Mesw=s64","userId":"03106579917275952793"}},"outputId":"a600f085-78c7-4c7b-f6fb-9a69f66185b8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8645833333333334"]},"metadata":{},"execution_count":100}],"source":["# auc 출력\n","\n","roc_auc_score(probs, labels)"]},{"cell_type":"code","source":[""],"metadata":{"id":"iSH_DurxbvF8"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"9팀_이경찬_W2_4.ipynb의 사본","provenance":[{"file_id":"1GMf9G_5NCz_a3AmqYJLch07XV4Nnd2Gp","timestamp":1648137456431},{"file_id":"1BB3Bdat2cOEJgTf0PRoLeooVB3vulxhH","timestamp":1646267529268}],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"98d18fb90f924fb8bde62dbf839523c1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cd849fb46b594a19ae9230f7d0b132eb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5b77420dd0ec42d79f7ac0226e61a10e","IPY_MODEL_5d60ced1c1e14e43b2bdbf5ae86916a9","IPY_MODEL_b02bde81162b40f59107d778b99c9ec1"]}},"cd849fb46b594a19ae9230f7d0b132eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5b77420dd0ec42d79f7ac0226e61a10e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_506df88525cb4350994377e316f16179","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_87f53651a97d426aab43a6d65ec1d26f"}},"5d60ced1c1e14e43b2bdbf5ae86916a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e0507c63a49d4e79a1779c562da629db","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":425,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":425,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3e57aabdcfc94eaf972fc353c2f3a0d2"}},"b02bde81162b40f59107d778b99c9ec1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3b50bdbd106547b9a26ae2078ff8f2c7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 425/425 [00:00&lt;00:00, 14.3kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_01c4b032116b49c6adfb6d9d5446f15d"}},"506df88525cb4350994377e316f16179":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"87f53651a97d426aab43a6d65ec1d26f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0507c63a49d4e79a1779c562da629db":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3e57aabdcfc94eaf972fc353c2f3a0d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b50bdbd106547b9a26ae2078ff8f2c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"01c4b032116b49c6adfb6d9d5446f15d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ceddce4bd1ae4940862fb58f960eb992":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cfe6f1b158d3475fad73ba1d756598d8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d3b8b1eaa08d4a509b7a3f9bad46c664","IPY_MODEL_5334d72753194928ae2d0c3c28a9ac68","IPY_MODEL_dc87f6a4063f4b46a476ceb8924614cb"]}},"cfe6f1b158d3475fad73ba1d756598d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d3b8b1eaa08d4a509b7a3f9bad46c664":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_63eee48e81724b86940c26f4127a1633","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2f260a136abb4da3be1abf4c45f87562"}},"5334d72753194928ae2d0c3c28a9ac68":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1409bc87fc3c4e1199c10d8fa7a6662a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":445025130,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":445025130,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ed7b22fec1bf4fe88ab7be8521cb1986"}},"dc87f6a4063f4b46a476ceb8924614cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6ec435164d604f77897e01e21eeaf5ab","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 424M/424M [00:07&lt;00:00, 57.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b72ae773a7004edc88430f8f0b106ac5"}},"63eee48e81724b86940c26f4127a1633":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2f260a136abb4da3be1abf4c45f87562":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1409bc87fc3c4e1199c10d8fa7a6662a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ed7b22fec1bf4fe88ab7be8521cb1986":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6ec435164d604f77897e01e21eeaf5ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b72ae773a7004edc88430f8f0b106ac5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}